{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Dataset Setup"
      ],
      "metadata": {
        "id": "E1Q4AmqJQjq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0lkgosKBJWNj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "from joblib import dump, load\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89_h5D2PQq1"
      },
      "source": [
        "## Resizing images and normalizing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AGI8TxBRPYCp"
      },
      "outputs": [],
      "source": [
        "transforms.resnet = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE16cKo-Q_GU"
      },
      "source": [
        "### Loading dataset (CIFAR-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k73ePpbLRHkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2093b672-b606-4e1f-8202-59a042d1e4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 40.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.resnet)\n",
        "testset_full = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ua6umg6SWkx"
      },
      "source": [
        "Selecting 500 training images and 100 test images per class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o6r_VujwSfbA"
      },
      "outputs": [],
      "source": [
        "def get_subset(dataset, indices):\n",
        "    target = np.array(dataset.targets)\n",
        "    selected_indices = []\n",
        "    for i in range(10):\n",
        "        i_indices = np.where(target == i)[0][:indices]\n",
        "        selected_indices.extend(i_indices)\n",
        "    return Subset(dataset, selected_indices)\n",
        "\n",
        "trainset = get_subset(trainset_full, 500)\n",
        "testset = get_subset(testset_full, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1G3867bV2Qb"
      },
      "source": [
        "### Loading pretrained ResNet-18 and removing the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "CuNFG3mYWQCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd25f1a2-435c-42a3-bc85-0fa7e8cbd913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
        "feature_extractor.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kMc7i6DXlw2"
      },
      "source": [
        "### Extract feature vector to get 512x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "zlJjGfXKa60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a520611f-0e56-4d68-a19f-546304e9afe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 512) (1000, 512)\n"
          ]
        }
      ],
      "source": [
        "def extract_features(dataset, model, batch_size=64):\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, batch_labels in dataloader:\n",
        "            outputs = model(images)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs)\n",
        "            labels.append(batch_labels)\n",
        "    features = torch.cat(features).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(trainset, feature_extractor)\n",
        "test_features, test_labels = extract_features(testset, feature_extractor)\n",
        "\n",
        "print(train_features.shape, test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwRh5vdOd-vT"
      },
      "source": [
        "## Using PCA to reduce the size of feature vector from 512x1 to 50x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tdUiyYSaeFl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf21b00-8304-434d-96f8-19aba4e172ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 50) (1000, 50)\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=50)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "test_features_pca = pca.transform(test_features)\n",
        "\n",
        "print(train_features_pca.shape, test_features_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to extract metrics"
      ],
      "metadata": {
        "id": "yutH1YxsheLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def metrics_row(y_true, y_pred, model_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='macro', zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Macro Precision\": prec,\n",
        "        \"Macro Recall\": rec,\n",
        "        \"Macro F1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "8T-oo3u8hkTC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Naive Bayes"
      ],
      "metadata": {
        "id": "R1_896xfQtku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEbqudEgkSz_"
      },
      "source": [
        "Part3.1.1 - Build confusion matrix (rows=true labels, cols=predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-p1SDVCVkJnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=10):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA2qpAbEkpsc"
      },
      "source": [
        "Part3.1.2 - Compute precision, recall, F1, and accuracy (macro + per-class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wrSfHN6WklkU"
      },
      "outputs": [],
      "source": [
        "def precision_recall_f1_from_cm(cm, eps=1e-12):\n",
        "    tp = np.diag(cm).astype(np.float64)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall    = tp / (tp + fn + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "\n",
        "    return {\n",
        "        \"per_class_precision\": precision,\n",
        "        \"per_class_recall\": recall,\n",
        "        \"per_class_f1\": f1,\n",
        "        \"macro_precision\": precision.mean(),\n",
        "        \"macro_recall\": recall.mean(),\n",
        "        \"macro_f1\": f1.mean(),\n",
        "        \"accuracy\": tp.sum() / cm.sum()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQs9SP0slDqh"
      },
      "source": [
        "Part 3.1.3 — Pretty-print evaluation report with metrics and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P1Jt_q3zk-ev"
      },
      "outputs": [],
      "source": [
        "def print_eval_report(name, y_true, y_pred, class_names=None):\n",
        "    cm = confusion_matrix_np(y_true, y_pred, num_classes=10)\n",
        "    m = precision_recall_f1_from_cm(cm)\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {m['accuracy']:.4f}\")\n",
        "    print(f\"- Macro Precision: {m['macro_precision']:.4f}\")\n",
        "    print(f\"- Macro Recall   : {m['macro_recall']:.4f}\")\n",
        "    print(f\"- Macro F1       : {m['macro_f1']:.4f}\")\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [str(i) for i in range(10)]\n",
        "    for i, cname in enumerate(class_names):\n",
        "        p = m[\"per_class_precision\"][i]\n",
        "        r = m[\"per_class_recall\"][i]\n",
        "        f = m[\"per_class_f1\"][i]\n",
        "        print(f\"  class {i} ({cname}): P={p:.4f} R={r:.4f} F1={f:.4f}\")\n",
        "\n",
        "cifar10_classes = [\n",
        "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
        "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztI1l3YLmJ3a"
      },
      "source": [
        "# Part 3.2.1 — Define Gaussian Naive Bayes class (NumPy only implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xYYkJ6SHmHKu"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n",
        "    \"\"\"\n",
        "    NumPy-only Gaussian Naive Bayes:\n",
        "    - Estimate class priors, per-class means/variances\n",
        "    - Predict via log-likelihood + log-prior (argmax)\n",
        "    \"\"\"\n",
        "    def __init__(self, var_smoothing=1e-9):\n",
        "        self.var_smoothing = var_smoothing\n",
        "        self.classes_ = None\n",
        "        self.class_priors_ = None\n",
        "        self.means_ = None\n",
        "        self.vars_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        C = len(self.classes_)\n",
        "        N, D = X.shape\n",
        "\n",
        "        self.means_ = np.zeros((C, D), dtype=np.float64)\n",
        "        self.vars_  = np.zeros((C, D), dtype=np.float64)\n",
        "        self.class_priors_ = np.zeros(C, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            Xc = X[y == c]\n",
        "            self.means_[idx] = Xc.mean(axis=0)\n",
        "            self.vars_[idx]  = Xc.var(axis=0) + self.var_smoothing\n",
        "            self.class_priors_[idx] = len(Xc) / float(N)\n",
        "        return self\n",
        "\n",
        "    def _log_gaussian_likelihood(self, X):\n",
        "        X = np.asarray(X)\n",
        "        means = self.means_[None, :, :]   # (1, C, D)\n",
        "        vars_ = self.vars_[None, :, :]    # (1, C, D)\n",
        "        X_    = X[:, None, :]             # (N, 1, D)\n",
        "\n",
        "        log_term  = -0.5 * (np.log(2.0 * np.pi * vars_)).sum(axis=2)\n",
        "        quad_term = -0.5 * (((X_ - means) ** 2) / vars_).sum(axis=2)\n",
        "        return log_term + quad_term\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_like = self._log_gaussian_likelihood(X)\n",
        "        log_prior = np.log(self.class_priors_)[None, :]\n",
        "        scores = log_like + log_prior\n",
        "        idx = np.argmax(scores, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgGPBc48mSED"
      },
      "source": [
        "Part 3.2.2 — Fit method: estimate means, variances, and priors for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QDrL6Ws0mNAM"
      },
      "outputs": [],
      "source": [
        "def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        C = len(self.classes_)\n",
        "        N, D = X.shape\n",
        "\n",
        "        self.means_ = np.zeros((C, D), dtype=np.float64)\n",
        "        self.vars_  = np.zeros((C, D), dtype=np.float64)\n",
        "        self.class_priors_ = np.zeros(C, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            Xc = X[y == c]\n",
        "            self.means_[idx] = Xc.mean(axis=0)\n",
        "            self.vars_[idx]  = Xc.var(axis=0) + self.var_smoothing\n",
        "            self.class_priors_[idx] = len(Xc) / float(N)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAWNb20mYde"
      },
      "source": [
        "Part 3.2.3 — Log-likelihood computation and prediction using argmax of scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8mAJzXJCmZO1"
      },
      "outputs": [],
      "source": [
        "def _log_gaussian_likelihood(self, X):\n",
        "        X = np.asarray(X)\n",
        "        means = self.means_[None, :, :]   # (1, C, D)\n",
        "        vars_ = self.vars_[None, :, :]    # (1, C, D)\n",
        "        X_    = X[:, None, :]             # (N, 1, D)\n",
        "\n",
        "        log_term  = -0.5 * (np.log(2.0 * np.pi * vars_)).sum(axis=2)\n",
        "        quad_term = -0.5 * (((X_ - means) ** 2) / vars_).sum(axis=2)\n",
        "        return log_term + quad_term\n",
        "\n",
        "def predict(self, X):\n",
        "        log_like = self._log_gaussian_likelihood(X)\n",
        "        log_prior = np.log(self.class_priors_)[None, :]\n",
        "        scores = log_like + log_prior\n",
        "        idx = np.argmax(scores, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_M71BKCtF2T"
      },
      "source": [
        "Part 3.3.1 — Fit (train) the scratch GNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q3Ry4ECkstaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6e1eb8-94a5-457a-ba1d-c4b1b422cdc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scratch GNB fitted on PCA-50 features.\n"
          ]
        }
      ],
      "source": [
        "gnb_scratch = GaussianNaiveBayes(var_smoothing=1e-9)\n",
        "gnb_scratch.fit(train_features_pca, train_labels)\n",
        "print(\"Scratch GNB fitted on PCA-50 features.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcF6dwZ-tOoX"
      },
      "source": [
        "Part 3.3.2 — Predict & print evaluation report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "frybcqeLtLXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d52a54-9551-4620-d73f-2f463a22e165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Gaussian Naive Bayes (Scratch, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[81  1  0  2  0  0  1  0 11  4]\n",
            " [ 3 88  0  2  1  0  0  0  0  6]\n",
            " [ 6  0 61  8 10  3 11  0  1  0]\n",
            " [ 1  0  5 76  3  9  6  0  0  0]\n",
            " [ 1  0  4  6 75  3  3  7  1  0]\n",
            " [ 0  1  5 12  3 75  2  2  0  0]\n",
            " [ 2  0  4  6  5  2 80  1  0  0]\n",
            " [ 1  1  0  4  7  5  0 81  1  0]\n",
            " [ 8  0  1  0  1  0  0  0 87  3]\n",
            " [ 5  3  0  2  0  0  0  1  1 88]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.7920\n",
            "- Macro Precision: 0.7962\n",
            "- Macro Recall   : 0.7920\n",
            "- Macro F1       : 0.7923\n",
            "  class 0 (airplane): P=0.7500 R=0.8100 F1=0.7788\n",
            "  class 1 (automobile): P=0.9362 R=0.8800 F1=0.9072\n",
            "  class 2 (bird): P=0.7625 R=0.6100 F1=0.6778\n",
            "  class 3 (cat): P=0.6441 R=0.7600 F1=0.6972\n",
            "  class 4 (deer): P=0.7143 R=0.7500 F1=0.7317\n",
            "  class 5 (dog): P=0.7732 R=0.7500 F1=0.7614\n",
            "  class 6 (frog): P=0.7767 R=0.8000 F1=0.7882\n",
            "  class 7 (horse): P=0.8804 R=0.8100 F1=0.8437\n",
            "  class 8 (ship): P=0.8529 R=0.8700 F1=0.8614\n",
            "  class 9 (truck): P=0.8713 R=0.8800 F1=0.8756\n"
          ]
        }
      ],
      "source": [
        "pred_scratch = gnb_scratch.predict(test_features_pca)\n",
        "print_eval_report(\n",
        "    \"Gaussian Naive Bayes (Scratch, PCA-50)\",\n",
        "    test_labels, pred_scratch,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "UuHSVnZDjrbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_gnb_scratch = metrics_row(test_labels, pred_scratch, \"Naive Bayes (Scratch)\")"
      ],
      "metadata": {
        "id": "Uiv_L0CujtwO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPLkEWktPvB"
      },
      "source": [
        "## Part 3.4.1 — Gaussian Naive Bayes (scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "q6_h5ACOtP9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26f16c7-e0ec-494f-86be-667eddc6a696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn GNB fitted on PCA-50 features.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb_sklearn = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_sklearn.fit(train_features_pca, train_labels)\n",
        "print(\"sklearn GNB fitted on PCA-50 features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU0oesJqtq6f"
      },
      "source": [
        "Part 3.4.2 — Predict & print evaluation report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9ZwIyDdstrM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18aea630-2243-46df-c011-1b5924fb3510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Gaussian Naive Bayes (scikit-learn, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[81  1  0  2  0  0  1  0 11  4]\n",
            " [ 3 88  0  2  1  0  0  0  0  6]\n",
            " [ 6  0 61  8 10  3 11  0  1  0]\n",
            " [ 1  0  5 76  3  9  6  0  0  0]\n",
            " [ 1  0  4  6 75  3  3  7  1  0]\n",
            " [ 0  1  5 12  3 75  2  2  0  0]\n",
            " [ 2  0  4  6  5  2 80  1  0  0]\n",
            " [ 1  1  0  4  7  5  0 81  1  0]\n",
            " [ 8  0  1  0  1  0  0  0 87  3]\n",
            " [ 5  3  0  2  0  0  0  1  1 88]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.7920\n",
            "- Macro Precision: 0.7962\n",
            "- Macro Recall   : 0.7920\n",
            "- Macro F1       : 0.7923\n",
            "  class 0 (airplane): P=0.7500 R=0.8100 F1=0.7788\n",
            "  class 1 (automobile): P=0.9362 R=0.8800 F1=0.9072\n",
            "  class 2 (bird): P=0.7625 R=0.6100 F1=0.6778\n",
            "  class 3 (cat): P=0.6441 R=0.7600 F1=0.6972\n",
            "  class 4 (deer): P=0.7143 R=0.7500 F1=0.7317\n",
            "  class 5 (dog): P=0.7732 R=0.7500 F1=0.7614\n",
            "  class 6 (frog): P=0.7767 R=0.8000 F1=0.7882\n",
            "  class 7 (horse): P=0.8804 R=0.8100 F1=0.8437\n",
            "  class 8 (ship): P=0.8529 R=0.8700 F1=0.8614\n",
            "  class 9 (truck): P=0.8713 R=0.8800 F1=0.8756\n",
            "[Saved] Scratch Naive Bayes model → Models/naive_bayes_scratch.pkl\n",
            "[Saved] Scikit-Learn Naive Bayes model → Models/naive_bayes_sklearn.pkl\n"
          ]
        }
      ],
      "source": [
        "pred_sklearn = gnb_sklearn.predict(test_features_pca)\n",
        "print_eval_report(\n",
        "    \"Gaussian Naive Bayes (scikit-learn, PCA-50)\",\n",
        "    test_labels, pred_sklearn,\n",
        "    class_names=cifar10_classes\n",
        ")\n",
        "# === Save Scratch Naive Bayes ===\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "nb_scratch_state = {\n",
        "    \"means\": gnb_scratch.means_,\n",
        "    \"vars\": gnb_scratch.vars_,\n",
        "    \"class_priors\": gnb_scratch.class_priors_,\n",
        "}\n",
        "\n",
        "save_path_scratch = \"Models/naive_bayes_scratch.pkl\"\n",
        "os.makedirs(os.path.dirname(save_path_scratch), exist_ok=True)\n",
        "\n",
        "with open(save_path_scratch, \"wb\") as f:\n",
        "    pickle.dump(nb_scratch_state, f)\n",
        "\n",
        "print(f\"[Saved] Scratch Naive Bayes model → {save_path_scratch}\")\n",
        "\n",
        "# === Save Scikit-Learn Naive Bayes ===\n",
        "nb_sklearn_state = {\n",
        "    \"model\": gnb_sklearn\n",
        "}\n",
        "\n",
        "save_path_sklearn = \"Models/naive_bayes_sklearn.pkl\"\n",
        "os.makedirs(os.path.dirname(save_path_sklearn), exist_ok=True)\n",
        "\n",
        "with open(save_path_sklearn, \"wb\") as f:\n",
        "    pickle.dump(nb_sklearn_state, f)\n",
        "\n",
        "print(f\"[Saved] Scikit-Learn Naive Bayes model → {save_path_sklearn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "CRyK4M0H8WbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_gnb_sklearn = metrics_row(test_labels, pred_sklearn, \" Scikit’s Gaussian Naive Bayes\")"
      ],
      "metadata": {
        "id": "3rXfAGLO8X1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Decision Tree Implementing Gini Impurity"
      ],
      "metadata": {
        "id": "9HAZeE_ZCJeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(labels):\n",
        "    classes, counts = np.unique(labels, return_counts=True)\n",
        "    probs = counts / len(labels) #Probability\n",
        "    return 1 - np.sum(probs ** 2) #Gini impurity formula"
      ],
      "metadata": {
        "id": "VW8Zz2ksCUtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset"
      ],
      "metadata": {
        "id": "5-lJTO29DhTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, y, feature_idx, threshold): #x=features of the dataset, y=labels/targets\n",
        "    left_indices = X[:, feature_idx] <= threshold\n",
        "    right_indices = X[:, feature_idx] > threshold\n",
        "    return X[left_indices], y[left_indices], X[right_indices], y[right_indices]"
      ],
      "metadata": {
        "id": "SeodytsSDjmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best split"
      ],
      "metadata": {
        "id": "g-HI23HLFAKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(X, y):\n",
        "    best_gini = 1\n",
        "    best_feature_idx = None\n",
        "    best_threshold = None\n",
        "\n",
        "    n_features = X.shape[1] #number of features in the dataset\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        thresholds = np.unique(X[:, feature_idx])\n",
        "        for threshold in thresholds:\n",
        "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature_idx, threshold)\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "            gini_left = gini_impurity(y_left)\n",
        "            gini_right = gini_impurity(y_right)\n",
        "            gini_weight = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)\n",
        "\n",
        "            if gini_weight < best_gini:\n",
        "                best_gini = gini_weight\n",
        "                best_feature_idx = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_idx, best_threshold"
      ],
      "metadata": {
        "id": "2S8N1BzjFCo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the decision tree"
      ],
      "metadata": {
        "id": "Uw4Tdy65Haf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, depth=0, max_depth=50):\n",
        "        self.max_depth = max_depth\n",
        "        self.depth = depth\n",
        "        self.feature_idx = None #index that is used to split\n",
        "        self.threshold = None\n",
        "        self.left = None #left child\n",
        "        self.right = None #right child\n",
        "        self.value = None\n",
        "\n",
        "#Creates a new node for that part of the tree\n",
        "def buildTree(X, y, depth=0, max_depth=50):\n",
        "    node = DecisionTree(depth, max_depth)\n",
        "\n",
        "    #stop condition (pure node or max depth reaqched)\n",
        "    if len(np.unique(y)) == 1 or depth >= max_depth:\n",
        "        node.value = np.bincount(y).argmax()\n",
        "        return node\n",
        "\n",
        "    #best split (picks feature and threashold with lowest Gini impurity)\n",
        "    feature_idx, threshold = best_split(X, y)\n",
        "    if feature_idx is None:\n",
        "        node.value = np.bincount(y).argmax()\n",
        "        return node\n",
        "\n",
        "    node.feature_idx = feature_idx\n",
        "    node.threshold = threshold\n",
        "\n",
        "    X_left, y_left, X_right, y_right = split_dataset(X, y, feature_idx, threshold)\n",
        "    node.left = buildTree(X_left, y_left, depth + 1, max_depth)\n",
        "    node.right = buildTree(X_right, y_right, depth + 1, max_depth)\n",
        "\n",
        "    return node"
      ],
      "metadata": {
        "id": "Vz4us0rIHdYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction of Decision Tree"
      ],
      "metadata": {
        "id": "QJ2qqaTVNAA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(node, X):\n",
        "    y_prediction = []\n",
        "    for x in X: #start at the root node\n",
        "        current = node\n",
        "\n",
        "        #traversing the tree (until leaf is reached)\n",
        "        while current.value is None:\n",
        "            if x[current.feature_idx] <= current.threshold:\n",
        "                current = current.left\n",
        "            else:\n",
        "                current = current.right\n",
        "        y_prediction.append(current.value) #take the class label as prediction if leaf node is reached\n",
        "    return np.array(y_prediction) #returns all the predictions"
      ],
      "metadata": {
        "id": "Vl-5ERzdNGV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics for the Decision Tree"
      ],
      "metadata": {
        "id": "DtxoRU8WOhe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusionMatrix(y_true, y_prediction, num_classes=10):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int) #initializing num_class matrix to zero\n",
        "    for t, p in zip(y_true, y_prediction):\n",
        "        cm[t, p] += 1 #Rows = actual labels, Columns = predicted labels\n",
        "    return cm\n",
        "\n",
        "def computeMetrics(cm):\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    precision = np.diag(cm) / np.maximum(cm.sum(axis=0), 1)\n",
        "    recall = np.diag(cm) / np.maximum(cm.sum(axis=1), 1)\n",
        "    f1 = 2 * precision * recall / np.maximum(precision + recall, 1e-6)\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "zu37Ii_5Qgs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing"
      ],
      "metadata": {
        "id": "kuRmeloM7sC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"decision_tree_scratch.pkl\"\n",
        "\n",
        "#Train if not saved (Saving model)\n",
        "if not os.path.exists(model_path):\n",
        "  tree = buildTree(train_features_pca, train_labels, max_depth=10)\n",
        "  with open(model_path, \"wb\") as f:\n",
        "    pickle.dump(tree, f)\n",
        "else:\n",
        "  with open(model_path, \"rb\") as f:\n",
        "    tree = pickle.load(f)\n",
        "\n",
        "#Evaluation\n",
        "y_prediction = predict(tree, test_features_pca)\n",
        "\n",
        "cm = confusionMatrix(test_labels, y_prediction)\n",
        "\n",
        "accuracy, precision, recall, f1 = computeMetrics(cm)\n",
        "\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== Decision Tree (Scratch, PCA-50) =====\")\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "\n",
        "macro_precision = np.mean(precision)\n",
        "macro_recall = np.mean(recall)\n",
        "macro_f1 = np.mean(f1)\n",
        "\n",
        "print(\"\\nOverall:\")\n",
        "print(f\"- Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"- Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"- Macro Recall   : {macro_recall:.4f}\")\n",
        "print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  class {i} ({name}): \"\n",
        "          f\"P={precision[i]:.4f} R={recall[i]:.4f} F1={f1[i]:.4f}\")"
      ],
      "metadata": {
        "id": "a4C_0qC77_vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "zGeuBrlDkDus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_dt_scratch = metrics_row(test_labels, y_prediction, \"Decision Tree (Scratch)\")"
      ],
      "metadata": {
        "id": "tJ9I_UJCkFy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn Decision Tree"
      ],
      "metadata": {
        "id": "AF2YRcZSVd_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"decision_tree_sklearn.pkl\"\n",
        "\n",
        "#Train if not saved (Saving model)\n",
        "if not os.path.exists(model_path):\n",
        "  clf = DecisionTreeClassifier(criterion = 'gini', max_depth=10, random_state=42)\n",
        "  clf.fit(train_features_pca, train_labels)\n",
        "  dump(clf, model_path)\n",
        "else:\n",
        "  clf = load(model_path)\n",
        "\n",
        "y_prediction_sklearn = clf.predict(test_features_pca)\n",
        "\n",
        "#computing matrix from sklearn\n",
        "cm_sklearn = confusion_matrix(test_labels, y_prediction_sklearn)\n",
        "accuracy_sklearn = accuracy_score(test_labels, y_prediction_sklearn)\n",
        "precision_sklearn = precision_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "recall_sklearn = recall_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "f1_sklearn = f1_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "\n",
        "#printing information\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== Decision Tree (Scikit-learn, PCA-50) =====\")\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm_sklearn)\n",
        "\n",
        "macro_precision = np.mean(precision_sklearn)\n",
        "macro_recall = np.mean(recall_sklearn)\n",
        "macro_f1 = np.mean(f1_sklearn)\n",
        "\n",
        "print(\"\\nOverall:\")\n",
        "print(f\"- Accuracy       : {accuracy_sklearn:.4f}\")\n",
        "print(f\"- Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"- Macro Recall   : {macro_recall:.4f}\")\n",
        "print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  class {i} ({name}): \"\n",
        "          f\"P={precision_sklearn[i]:.4f} R={recall_sklearn[i]:.4f} F1={f1_sklearn[i]:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EqtkSc0cVivz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "_3eLNmht8HnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_dt_sklearn = metrics_row(test_labels, y_prediction_sklearn, \"Scikit’s implementation of a Decision Tree\")"
      ],
      "metadata": {
        "id": "rmb0qX3e8JiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "kFE8yV1wQQTc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "498c1f20"
      },
      "source": [
        "## 5.1: Define the MLP architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d2c72c1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 512)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.bn_2 = nn.BatchNorm1d(512)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867e0c3d"
      },
      "source": [
        "Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b2c3935"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "mlp_initial_model_path = \"mlp_initial.pth\"\n",
        "\n",
        "model = MLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "if os.path.exists(mlp_initial_model_path):\n",
        "    print(\"Loading saved MLP model...\")\n",
        "    model.load_state_dict(torch.load(mlp_initial_model_path, map_location=device))\n",
        "    model.eval() # Set to evaluation mode after loading\n",
        "    print(\"MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Multilayer Perceptron (MLP) created. Model will be trained and saved.\")\n",
        "\n",
        "model.to(device) # Move model to device\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"Training with PyTorch's CrossEntropyLoss\")\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "print(\"Using SGD optimizer with momentum of 0.9\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate confusion matrix for initial MLP"
      ],
      "metadata": {
        "id": "bjIHvwZSYVr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Ensure mlp_initial_model_path and device are defined if running this cell independently\n",
        "# (they should be defined in the previous cell)\n",
        "mlp_initial_model_path = \"mlp_initial.pth\"\n",
        "\n",
        "# Convert training data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "\n",
        "# Define num_epochs and batch_size\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if not os.path.exists(mlp_initial_model_path):\n",
        "    print(\"Starting training for initial MLP...\")\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # Move data to device\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "            outputs = model(inputs) # Forward pass\n",
        "            loss = criterion(outputs, labels) # Calculate loss\n",
        "            loss.backward() # Backpropagation\n",
        "            optimizer.step() # Update weights\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(model.state_dict(), mlp_initial_model_path)\n",
        "    print(f\"MLP model saved to {mlp_initial_model_path}\")\n",
        "else:\n",
        "    print(\"Skipping training as MLP model already exists and was loaded.\")\n",
        "\n",
        "# Convert test data to PyTorch tensors and move to device\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_features_pca_tensor)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "# Convert predictions and labels to numpy array (back to CPU for numpy conversion if on GPU)\n",
        "pred_mlp_initial = predicted.cpu().numpy()\n",
        "\n",
        "# Lines to display the results (Confusion Matrix and metrics)\n",
        "print_eval_report(\n",
        "    \"MLP (Initial, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(), # Pass labels from CPU tensor\n",
        "    pred_mlp_initial,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "GOlX40ikYVRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "Llh6jVPU85L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_initial = metrics_row(test_labels, pred_mlp_initial, \"MLP\")"
      ],
      "metadata": {
        "id": "woVtd49w86nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.1: Adding layers"
      ],
      "metadata": {
        "id": "9E38fAEmUlqx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D20rIrBwUNh"
      },
      "source": [
        "# Define an MLP with more layers\n",
        "class DeeperMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(DeeperMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 512)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.bn_2 = nn.BatchNorm1d(512)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(512, 256) # Added a new hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.layer_4 = nn.Linear(256, num_classes) # Output layer adjusted\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        return x\n",
        "\n",
        "print(\"Deeper Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the deeper MLP model\n",
        "deep_mlp_model_path = \"deeper_mlp.pth\"\n",
        "deeper_model = DeeperMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "deeper_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the deeper model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_deeper = optim.SGD(deeper_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(deep_mlp_model_path):\n",
        "    print(\"Loading saved Deeper MLP model...\")\n",
        "    deeper_model.load_state_dict(torch.load(deep_mlp_model_path, map_location=device))\n",
        "    deeper_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Deeper MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Deeper MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        deeper_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_deeper.zero_grad()\n",
        "            outputs = deeper_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_deeper.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(deeper_model.state_dict(), deep_mlp_model_path)\n",
        "    print(f\"Deeper MLP model saved to {deep_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the deeper model\n",
        "deeper_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = deeper_model(test_features_pca_tensor)\n",
        "    _, predicted_deeper = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the deeper MLP\n",
        "pred_deeper_mlp = predicted_deeper.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Deeper, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_deeper_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "YOsIBx4R9GWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_deeper = metrics_row(test_labels, pred_deeper_mlp, \"MLP (Deeper, PCA-50)\")"
      ],
      "metadata": {
        "id": "tV5maTPf9H_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2: Removing layers"
      ],
      "metadata": {
        "id": "iffaHBamU2N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with fewer layers (e.g. one hidden layer)\n",
        "class ShallowerMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ShallowerMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 256) # Reduced hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(256, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Shallower Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the shallower MLP model\n",
        "shallower_mlp_model_path = \"shallower_mlp.pth\"\n",
        "shallower_model = ShallowerMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "shallower_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the shallower model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_shallower = optim.SGD(shallower_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(shallower_mlp_model_path):\n",
        "    print(\"Loading saved Shallower MLP model...\")\n",
        "    shallower_model.load_state_dict(torch.load(shallower_mlp_model_path, map_location=device))\n",
        "    shallower_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Shallower MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Shallower MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        shallower_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_shallower.zero_grad()\n",
        "            outputs = shallower_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_shallower.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(shallower_model.state_dict(), shallower_mlp_model_path)\n",
        "    print(f\"Shallower MLP model saved to {shallower_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the shallower model\n",
        "shallower_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = shallower_model(test_features_pca_tensor)\n",
        "    _, predicted_shallower = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the shallower MLP\n",
        "pred_shallower_mlp = predicted_shallower.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Shallower, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_shallower_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "jOjmSVAdkjr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "B9richDg9Uk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_shallower = metrics_row(test_labels, pred_shallower_mlp, \"MLP (Shallower, PCA-50)\")"
      ],
      "metadata": {
        "id": "41UXRO6B9Wm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.1: Larger Hidden Layer"
      ],
      "metadata": {
        "id": "NsYWt-gPVEhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with larger hidden layers (e.g. 1024 units)\n",
        "class WiderMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(WiderMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 1024) # Increased hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(1024, 1024) # Increased hidden layer size\n",
        "        self.bn_2 = nn.BatchNorm1d(1024)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(1024, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "print(\"Wider Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the wider MLP model\n",
        "wider_mlp_model_path = \"wider_mlp.pth\"\n",
        "wider_model = WiderMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wider_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the wider model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_wider = optim.SGD(wider_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(wider_mlp_model_path):\n",
        "    print(\"Loading saved Wider MLP model...\")\n",
        "    wider_model.load_state_dict(torch.load(wider_mlp_model_path, map_location=device))\n",
        "    wider_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Wider MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Wider MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        wider_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_wider.zero_grad()\n",
        "            outputs = wider_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_wider.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(wider_model.state_dict(), wider_mlp_model_path)\n",
        "    print(f\"Wider MLP model saved to {wider_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the wider model\n",
        "wider_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = wider_model(test_features_pca_tensor)\n",
        "    _, predicted_wider = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the wider MLP\n",
        "pred_wider_mlp = predicted_wider.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Wider, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_wider_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "0sKYJiLqlM0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "FXoC9J9N9eTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_wider = metrics_row(test_labels, pred_wider_mlp, \"MLP (Wider, PCA-50)\")"
      ],
      "metadata": {
        "id": "BlCgegvK9fvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.2: Smaller Hidden Layer"
      ],
      "metadata": {
        "id": "hghMs375VHO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with smaller hidden layers (e.g. 128 units)\n",
        "class NarrowerMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NarrowerMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 128) # Decreased hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(128, 128) # Decreased hidden layer size\n",
        "        self.bn_2 = nn.BatchNorm1d(128)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(128, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "print(\"Narrower Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the narrower MLP model\n",
        "narrower_mlp_model_path = \"narrower_mlp.pth\"\n",
        "narrower_model = NarrowerMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "narrower_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the narrower model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_narrower = optim.SGD(narrower_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(narrower_mlp_model_path):\n",
        "    print(\"Loading saved Narrower MLP model...\")\n",
        "    narrower_model.load_state_dict(torch.load(narrower_mlp_model_path, map_location=device))\n",
        "    narrower_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Narrower MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Narrower MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        narrower_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_narrower.zero_grad()\n",
        "            outputs = narrower_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_narrower.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(narrower_model.state_dict(), narrower_mlp_model_path)\n",
        "    print(f\"Narrower MLP model saved to {narrower_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the narrower model\n",
        "narrower_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = narrower_model(test_features_pca_tensor)\n",
        "    _, predicted_narrower = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the narrower MLP\n",
        "pred_narrower_mlp = predicted_narrower.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Narrower, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_narrower_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "HevvNWFGm0SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "5Q42l-8y9wgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_narrower = metrics_row(test_labels, pred_narrower_mlp, \"MLP (Narrower, PCA-50)\")"
      ],
      "metadata": {
        "id": "5vGfHsky9x0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "f6cpBCyO-EHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1: Implementing and training a VGG11 net"
      ],
      "metadata": {
        "id": "xZhvFqjJ-z-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `torch.nn.CrossEntropyLoss`, and optimize using SGD optimizer with `momentum=0.9`"
      ],
      "metadata": {
        "id": "4Y0LqrKg-9Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing VGG11 - according to assignment description"
      ],
      "metadata": {
        "id": "9r5tQFjCpTTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11, self).__init__()\n",
        "    #extracting image features\n",
        "    self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "axdbMIY2pW7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Data for CNN (resizing to 32x32)"
      ],
      "metadata": {
        "id": "JE82L2Gm1ngj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "39lqp42S1wtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the CNN"
      ],
      "metadata": {
        "id": "p4spd-igBW1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11(num_classes=10).to(device)\n",
        "\n",
        "#where to store the model\n",
        "model_path = \"vgg11_base.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vgg11_base.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "#Train from scratch\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "UeTNSVGYBY5z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of CNN"
      ],
      "metadata": {
        "id": "U369LpsB61yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn, preds_cnn = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "#collecting metric\n",
        "row_cnn = metrics_row(labels_cnn, preds_cnn, \"CNN (VGG11)\")\n"
      ],
      "metadata": {
        "id": "Bl9y5DMe63oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2: Adding convolutional layers"
      ],
      "metadata": {
        "id": "yjibcjsf_XJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_Add(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            # added an extra layer\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # final feature map = 512×2×2 = 2048\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 2 * 2, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "l_N0Dr5fNP8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data after adding a layer"
      ],
      "metadata": {
        "id": "0Iihcw_pxSRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "E7pEnykOxYAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN after adding layer"
      ],
      "metadata": {
        "id": "WiaPwFU42Ia8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_Add(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_add.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_add.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "lIlcsIX72LB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation CNN after adding layer"
      ],
      "metadata": {
        "id": "TWF_eRsIAbPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11-Added, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_add, preds_cnn_add = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_add    = metrics_row(labels_cnn_add, preds_cnn_add, \"CNN (add)\")\n"
      ],
      "metadata": {
        "id": "HR1ssLzQAdrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3: Removing convolutional layers"
      ],
      "metadata": {
        "id": "itWicRDo_dfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_Remove(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11_Remove, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "      nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "      #removed 3 layers\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(512*4*4, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "      nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "      nn.Linear(4096, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "id": "6hptIWaHOepb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data after removing layer"
      ],
      "metadata": {
        "id": "CZO8Nrd5GVpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "hbJJtX0kGZHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN after removing layer"
      ],
      "metadata": {
        "id": "VHnI1VKCHaeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_Remove(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_remove.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_remove.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "qn6AbKMxI4Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating CNN after removing layer"
      ],
      "metadata": {
        "id": "deScbokpHsfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11-Remove, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_remove, preds_cnn_remove = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_remove = metrics_row(labels_cnn_remove, preds_cnn_remove, \"CNN (remove)\")\n"
      ],
      "metadata": {
        "id": "Q5ot0a8JHvQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4: Larger kernel size"
      ],
      "metadata": {
        "id": "U-FpmLSs_hEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel size of 5x5"
      ],
      "metadata": {
        "id": "JTIT3RiJXC_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_kernel5(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11_kernel5, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "            #Chaning the kernel size to 5x5\n",
        "            nn.Conv2d(3, 64, 5, 1, 2), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 5, 1, 2), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 5, 1, 2), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 5, 1, 2), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "06HdLIBcXM56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for kernel 5x5"
      ],
      "metadata": {
        "id": "pf2rI0axX9y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "aLEN59XBYAnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN with 5x5 kernel size"
      ],
      "metadata": {
        "id": "mI6dR3F3YoOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_kernel5(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_k5.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_k5.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NG4Ibsp_Ythe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the CNN with kernel size 5x5"
      ],
      "metadata": {
        "id": "JjjYYtAkYyBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11_Kernel5, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_k5, preds_cnn_k5 = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_k5      = metrics_row(labels_cnn_k5, preds_cnn_k5, \"CNN (Kernel 5x5)\")"
      ],
      "metadata": {
        "id": "PdmkXbhIY0my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5: Smaller kernel size"
      ],
      "metadata": {
        "id": "m2QfRA_5_qNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kept the kernel size 3x3 as small, which was already implemented above"
      ],
      "metadata": {
        "id": "BwqvygAWdyF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7: Evaluation Table"
      ],
      "metadata": {
        "id": "0W_9adGYnrv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Evaluation Table"
      ],
      "metadata": {
        "id": "IuflfyWmnvLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [\n",
        "    row_gnb_scratch,\n",
        "    row_gnb_sklearn,\n",
        "    row_dt_scratch,\n",
        "    row_dt_sklearn,\n",
        "    row_mlp_initial,\n",
        "    row_mlp_deeper,\n",
        "    row_mlp_shallower,\n",
        "    row_mlp_wider,\n",
        "    row_mlp_narrower,\n",
        "    row_cnn,\n",
        "    row_cnn_add,\n",
        "    row_cnn_remove,\n",
        "    row_cnn_k5\n",
        "]\n",
        "\n",
        "df_results = pd.DataFrame(rows)\n",
        "df_results"
      ],
      "metadata": {
        "id": "SBvwv-_WoLQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E1Q4AmqJQjq6",
        "f6cpBCyO-EHy",
        "m2QfRA_5_qNJ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}